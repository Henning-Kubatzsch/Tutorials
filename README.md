# ML & GPT Tutorial Portfolio

Welcome to my personal repository for deep learning and NLP tutorials.  
This repo documents my journey of understanding and implementing core concepts behind large language models ‚Äì from scratch, using PyTorch and Python.

## üìö Tutorials Covered

This collection includes full walkthroughs, annotated notebooks, and experiments based on tutorials by Andrej Karpathy and others. All code is written and commented by me to deepen understanding.

| # | Tutorial                     | Topics Covered                                                 |
|---|------------------------------|----------------------------------------------------------------|
| 1 | **Micrograd**                | Autograd from scratch, scalar backprop                        |
| 2 | **Makemore**                 | Char-level LMs, MLPs, softmax, initialization, training        |
| 3 | **Makemore Advanced**        | BatchNorm, weight scaling, gradient flow                      |
| 4 | **Bigram GPT**               | First transformer architecture, attention from scratch         |
| 5 | **Self-Attention Head**      | Keys, queries, values, causality                              |
| 6 | **Multi-Head Attention**     | Parallel heads, concatenation, projection                     |
| 7 | **Let's Build GPT**          | Full transformer block, layer norm, causal masking, training  |

## Why I built this

I wanted to:
- **Understand LLMs from the ground up** ‚Äì not just use them via APIs
- **Train and fine-tune models** later on for real-world tasks (especially EdTech)
- Build a strong base for working in ML/NLP-focused teams

Each notebook is written for **learning, experimentation, and later reference**.  
This repo is also part of my **portfolio** to showcase technical skills and learning depth.

## üõ† How to Use This Repo

Each subfolder contains:
- A clean, runnable notebook (Colab-ready)
- My notes and inline explanations
- Experiments and visualizations (loss, gradients, distributions)

You can:
- Use it to revisit concepts like backpropagation or attention
- Fork and extend it into your own transformer implementations
- Get inspired for educational or NLP projects

## What‚Äôs next

I am currently working on:
- Fine-tuning projects on domain-specific data
- LLM-based learning assistants (EdTech prototypes)
- Tools to visualize attention, saturation, and gradient flow

## Contact

Feel free to reach out ‚Äì I‚Äôm open to collaboration and looking for opportunities in:
- NLP / AI in education
- ML engineering
- Applied research

üìß henningkb@outlook.com
üåê [GitHub](https://github.com/Henning-Kubatzsch)  
üåç [LinkedIn](https://www.linkedin.com/in/henning-kubatzsch-632353324/)

---

Thanks for visiting!
