# ğŸ§  ML & GPT Tutorial Portfolio

Welcome to my personal repository for deep learning and NLP tutorials.  
This repo documents my journey of understanding and implementing core concepts behind large language models â€“ from scratch, using PyTorch and Python.

## ğŸ“š Tutorials Covered

This collection includes full walkthroughs, annotated notebooks, and experiments based on tutorials by Andrej Karpathy and others. All code is written and commented by me to deepen understanding.

| # | Tutorial                     | Topics Covered                                                 |
|---|------------------------------|----------------------------------------------------------------|
| 1 | **Micrograd**                | Autograd from scratch, scalar backprop                        |
| 2 | **Makemore**                 | Char-level LMs, MLPs, softmax, initialization, training        |
| 3 | **Makemore Advanced**        | BatchNorm, weight scaling, gradient flow                      |
| 4 | **Bigram GPT**               | First transformer architecture, attention from scratch         |
| 5 | **Self-Attention Head**      | Keys, queries, values, causality                              |
| 6 | **Multi-Head Attention**     | Parallel heads, concatenation, projection                     |
| 7 | **Let's Build GPT**          | Full transformer block, layer norm, causal masking, training  |

## ğŸ’¡ Why I built this

I wanted to:
- **Understand LLMs from the ground up** â€“ not just use them via APIs
- **Train and fine-tune models** later on for real-world tasks (especially EdTech)
- Build a strong base for working in ML/NLP-focused teams

Each notebook is written for **learning, experimentation, and later reference**.  
This repo is also part of my **portfolio** to showcase technical skills and learning depth.

## ğŸ›  How to Use This Repo

Each subfolder contains:
- ğŸ““ A clean, runnable notebook (Colab-ready)
- ğŸ“ My notes and inline explanations
- ğŸ”¬ Experiments and visualizations (loss, gradients, distributions)

You can:
- Use it to revisit concepts like backpropagation or attention
- Fork and extend it into your own transformer implementations
- Get inspired for educational or NLP projects

## ğŸš§ Whatâ€™s next

I am currently working on:
- ğŸ§ª Fine-tuning projects on domain-specific data
- ğŸ§‘â€ğŸ« LLM-based learning assistants (EdTech prototypes)
- ğŸ“ˆ Tools to visualize attention, saturation, and gradient flow

## ğŸ“¬ Contact

Feel free to reach out â€“ Iâ€™m open to collaboration and looking for opportunities in:
- NLP / AI in education
- ML engineering
- Applied research

ğŸ“§ henning.kubatzsch@...  
ğŸŒ [GitHub](https://github.com/Henning-Kubatzsch)  
ğŸŒ [LinkedIn](https://www.linkedin.com/in/henning-kubatzsch-632353324/)

---

Thanks for visiting!
